Good evening, I'm Andrew Ross Sorkin of "The New York Times" and CNBC filling in for Charlie Rose. One cannot overstate the power of technology to change the world. Jared Cohen is at the forefront of using technology to address some of our toughest challenges. He founded Google Ideas in 2010. He is now president of Jigsaw, its successor. The company is focused on a range of global security projects such as protecting people from online bullying and countering violent extremism. Before joining Google, Jared served at the State Department under Secretaries of State Condoleezza Rice and Hillary Clinton, where he pioneered the concept of digital diplomacy. I am pleased to have Jared Cohen on this program. 

Thanks for having me. 

Thank you for coming. Especially because you have just announced a new project, which I want to get to immediately, which is called Perspective, an idea about protecting people from bullying, from harassment online, which is something that we talk and think about so often, what are you doing? 

Well, we're all familiar with the toxicity problem online. It is one of the few challenges in the world that we all have had some kind of, at least unfortunately, had some kind of experience with. Perspective uses machine learning to detect that toxicity online, so that we can help publishers do something to mitigate it. 

So Elon Musk, on Twitter, at one point called it a hellscape. What has happened to culture online? 

I think this is humanity with us having more visibility into it. So the notion of toxicity, and people being mean to each other is not new. Throughout history and in every corner of the globe, even today, we're all experiencing instances where somebody derails a conversation. But online barriers of entry are lower, people can sort of scurry out of there faster. But I think the main issue is we have to understand that the toxicity we experience online, it has real-world implications. 

And we're not talking about fake news in this instance, which we can get to in a moment. We're talking about the idea of what we see on social media. What we see in the comment section of news stories, what we see on Twitter and elsewhere. That is what this is about. 

The internet is made up of publishers and platforms and publishers and platforms are eager to moderate comments. They all have their sort of terms of service, which are too long for people to read, but they're overwhelmed by the volume of nasty comments and they're looking for a way to deal with it. What we've built with Perspective is an opportunity for any publisher in the world to run all their comments through Perspective and in return, they get a toxicity score of 0 to 100. In this case, we've defined toxicity by "this language is likely to cause somebody to leave the conversation." 

Just to put it in context, what's going to happen? If I wrote something that was toxic, does it get eliminated from everyone else's feed? Is that what is happening here? 

That is the beauty of Perspective. What we're trying to do is help with the detection of what's toxic, not trying to make a determination of what one does with that score. So we all know you work at "The New York Times" as well, Andrew, so if you're "The New York Times" and you're using Perspective, you may make a determination that any toxicity score north of 75, you're going to remove from the comments. 

How do you define toxicity? 

So again, in this case, the way we define toxicity is "these comments are likely to cause somebody to leave the conversation." Now, the way in which we've done this is we have data sets from partners, research partners. We've also taken data and crowd sourced it to accredited, annotationed organizations and asked them the question, which of these comments do you think are likely to cause somebody to leave? And what's fascinating about this is there was greater agreement among annotators on what constitutes a toxic comment than there was on what constitutes a personal attack, something that is obscene, something that's off topic, something that's hate or harassment. 

How do you define hate or harassment? This is the issue. It's one thing to force a conversation offline or someone to leave the conversation. But are we talking about language, is this about abusive language or something more? 

In this case, we're talking about abusive language, right? So, you know, in order to use machine learning models for something like this, you need a data set that is large enough to train against. 

This is artificial intelligence that's literally reading thousands, if not millions of sentences. 

Yes, so we have, for instance, a large data set for "The New York Times" that has annotated -- that's basically annotated according to what's toxic. So the machine is able to learn based on that training data set. And then we have other research partners and so forth. So we're also then able to go out to communities of people and crowd source an answer to the question which of these comments are toxic? So human beings are literally reading these comments and determining, this one is toxic. 

How do you imagine the partners and publishers, that you will ultimately partner with, will use this, which is to say, there's a big first amendment issue? 

The publishers have already drawn a line or at least articulated a line in terms of what constitutes a violation of their policies. Again, the beauty of this is we're putting the power in the hands of the publishers. So in terms of ways that people might use this, so an obvious use case is for moderation, right? So there's not nearly enough human moderators to sift through the comments, whereas, if the human moderators are able to see in batch, these comments are all sort of over our toxicity threshold based on the number returned by Perspective, they can do it more efficiently. You can also imagine a situation where a publisher might decide, we want the people writing the comments to know that the comment in which they wrote crosses that toxicity threshold as well. 

So is the ultimate goal to change behavior? 

The ultimate goal is to get people to come back to these conversations, to have more civil discussion and, yeah, it is to change behavior. The reason we want to change behavior is, you know, if you look at what is happening, people are either shutting down -- publishers are either shutting down comments altogether, or they're so overwhelmed with negative discourse that all the sane people aren't participating. 

What about fake news, which is to say, what happens when somebody comments and lies? 

Fake news, the challenge is, it keeps getting defined in different ways. We have to be careful, first, to distinguish between news we don't like and fake news. 

But this system is not Donna be looking at that. 

This is not about dealing with fake news. This is solving one problem which is, people are nasty to each other in online discussions, and the arbiters of what discourse should be allowed or not allowed, which are the publishers, currently do not have detection tools to be able to scale the process of moderation. 

Last question on this particular issue, which is, when you look at why there is so much hate online, what do you think that is about? I always say, if real life was as terrible as it sometimes feels online, man, would this be a bad world to live in. 

Yeah, I'm not a sociologist, but I have some hypotheses about this. I think there is something about the distance between the person making the comment and the victim of the comment that lowers barriers of entry. It's why somebody is quicker to give somebody a digital black eye than they are to go up to somebody and punch them in the face. 

But is this about -- it's not even about anonymity because people put their names next to some pretty horrible stuff. 

I think having technology is something that creates space between the attacker and the attacked. Again, I think it makes people not count to ten, it makes people not think about consequences, and the consequences are less apparent, so I think that is part of it. I think also we're interacting with more people online in any given day than we're going to physically interact with in our entire life. So as the touch points go up, people agitate us. We're also experiencing more people who are not a part of the circle of individuals we interact with. By virtue of being online, you're in the same town square as people all across the world and you're going to encounter more and more voices that you don't want to hear. 

I imagine if there's a critique of this, it is going to be this is the ultimate form of big brother. Have you thought about that? 

So we thought a lot about this in the process of building Perspective and in the process of doing these models. And one of the first things we wanted to do is ensure we built something that was a tool that, you know, would empower publishers to be the ones ensuring discourse was constructive. We made sure that we built something where neither our data nor our models are things that can be used by governments. At the end of the day, the best protection for users is the fact that machine learning models themselves can't see through encryption, which helps address any concerns that one might have about surveillance. But ultimately, my view is if you look at the trends of conversations online, they literally get more toxic by the day. So you have 4.3 billion people online today. By 2020, you're gonna have 6.1 billion people online on smartphones. That's a lot of new people being added to conversations, by the way, in parts of the world where there is a lot of sort of political, ethnic, sectarian tensions, so we should expect this to get much worse if we don't do something about it. So the question is, you know, for a problem that we all agree is bad, right, which is people having way too easy a time being nasty at scale, if we can use machine learning to address that problem, why would we not give it a try? 

While we're here, I want you to put your state department hat on, which is to say, you spend a lot of time thinking about digital diplomacy, you also think about cyber attacks, our cyber capabilities and the capabilities of others. What do you worry about right now? 

I think the biggest thing that I worry about is the world is in a perpetual state of cyber warfare that is being driven by a group of countries of sort of varying forms of governance, that are sort of the cyber powers, and they are deploying cyber capabilities on a day-to-day basis against each other, against their populations, you know, out into the wild, and the rules of engagement, for what it means for states to interact with each other in the cyber domain, have not yet been written. 

Are there countries we should worry about that have been underappreciated in terms of what their cyber capabilities are, or cyber capabilities that they're trying to get? 

I think we have a pretty -- I mean, to me, the most powerful cyber countries that -- you have the U.S., U.K., Germany, Israel, China, Russia, North Korea has a very sophisticated capability, Iran has very sophisticated capability. But I think the real question is not which states are we failing to identify, I think it's more which states have demonstrated a habit of wreaking physical horror in the streets that do not possess a nefarious cyber capability and how will they go out and try to procure it? Either from one of the cyber powers that I mentioned, or from one of the underground criminal networks. What will they be able to trade in order to get it? 

We talked about rules, rules of the road in terms of how all these countries deal with each other and how they might retaliate. There, of course, have been multiple reports now that our intelligence officials believed that this election may have been hacked, or was hacked to some degree, by the Russians in terms of the e-mails that were ultimately discovered and released. What is the proportional response? This has been the big topic in Washington. 

There are no doctrines of proportional response for the cyber domain that I'm aware of, which is why we have no deterrents in the cyber domain. We have small examples, right, North Korea hacked Sony, the response is sanctions. The Obama administration in its final weeks kicked a bunch of Russian diplomats out and added additional sanctions. There were some indictments of some Iranians that had hacked a dam in the U.S., so you have these sort of small examples, but none of these examples are enough to really change state behavior. So if you're writing sort of new rules of the game, and by the way, we have decades of experience around deterrents and proportional response and so forth, but if you're to write rules that really create this kind of -- 

What do the rules look like? What should they look like? 

It has to be something that changes behavior. For instance, if the goal is to get, you know, a country to not conduct corporate espionage, what can you -- what threat can be sort of posed that would cause them to essentially change their behavior? This all gets much more difficult in a world where attribution is so difficult. So I believe right now that all wars will begin as cyber wars. What's so dangerous about cyber wars or cyber incursions or cyber invasions, whatever you want to call them, is you may not know that you were attacked for several years. So we're used to a world in which a physical attack happens, in that moment in the Situation Room or wherever else, that retaliatory moment, where you say, here's what happened to us, what are the options, what are we going to do, what are the consequences? What happens when that comes two years later? What happens when it comes three years later? And so much has changed, the political will is no longer there, everybody's kind of moved on, but you get this smoking gun information, what do you do if you're the leader of a country in that moment? 

I want to also talk about the role of corporations, specifically technology corporations in terms of the balance of power between states and companies. Harvard Business Review had a piece out in the fall that said every corporation needs a foreign policy. Do you think about a foreign policy for Google, for Alphabet? 

I think foreign policy is the wrong word because these companies are inherently global. I think the right way to think about it is each company has its mission statement, and it needs a strategy for applying that mission statement around the world. So companies, again, they have mission statements and they have a set of core values. Where this gets interesting is historically, even in the last ten years, there's been a whole host of international security issues that didn't seem relevant to the companies and I think what we're finding now is every physical challenge has a digital manifestation, all the problems from the physical world are now spilling over online and it's just much better for companies to assume whatever problem exists in the world is something that they should be proactive in thinking about and understanding how their mission statement and their values applies to it. 

What happens when your program decides to put a toxicity score on a politician, dare I say like President Trump, or somebody else? 

The models themselves are imperfect, right, so we're putting these models -- so with Perspective, there will be false positives. So we'll run comments through Perspective and the scores that come back against some of the comments, in some cases will be wrong, and in this case, the users themselves can correct it, and it will help improve the model. But this is also why you apply multiple models and it's also why you end up using lots of different data sets. 

Thank you for a conversation that was non-toxic. 

Thank you. 

Jared, thank you, appreciate it. 

